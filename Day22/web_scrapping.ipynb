{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport json\\n\\n# Send a GET request to the website\\nurl = \\'http://www.bu.edu/president/boston-university-facts-stats/\\'\\nresponse = requests.get(url)\\n\\n# Parse the HTML content using BeautifulSoup\\nsoup = BeautifulSoup(response.content, \\'html.parser\\')\\n\\n# Find the section containing the facts and stats\\nfacts_stats_section = soup.find(\\'section\\', class_=\\'facts-stats\\')\\n\\n# Extract the data from the section\\ndata = {}\\nfor row in facts_stats_section.find_all(\\'div\\', class_=\\'row\\'):\\n    key = row.find(\\'div\\', class_=\\'col-md-4\\').text.strip()\\n    value = row.find(\\'div\\', class_=\\'col-md-8\\').text.strip()\\n    data[key] = value\\n\\n# Save the data as a JSON file\\nwith open(\\'bu_facts_stats.json\\', \\'w\\') as file:\\n    json.dump(data, file, indent=4)\\n\\nprint(\"Data scraped and saved as bu_facts_stats.json\")\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'http://www.bu.edu/president/boston-university-facts-stats/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the section containing the facts and stats\n",
    "facts_stats_section = soup.find('section', class_='facts-stats')\n",
    "\n",
    "# Extract the data from the section\n",
    "data = {}\n",
    "for row in facts_stats_section.find_all('div', class_='row'):\n",
    "    key = row.find('div', class_='col-md-4').text.strip()\n",
    "    value = row.find('div', class_='col-md-8').text.strip()\n",
    "    data[key] = value\n",
    "\n",
    "# Save the data as a JSON file\n",
    "with open('bu_facts_stats.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Data scraped and saved as bu_facts_stats.json\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport json\\n\\n# Send a GET request to the website\\nurl = \\'http://www.bu.edu/president/boston-university-facts-stats/\\'\\nresponse = requests.get(url)\\n\\nif response.status_code == 200:\\n    # Parse the HTML content using BeautifulSoup\\nsoup = BeautifulSoup(response.content, \\'html.parser\\')\\n\\nelse:\\n    print(f\"Failed to retrieve the webpage. Status Code: {response.status_code}\")\\n\\n# Find the section containing the facts and stats\\nif facts_stats_section:\\n    # Extract the data from the section\\ndata = []\\ncategories = facts_stats_section.find_all(\\'div\\', class_=\\'col-md-4\\')\\nvalues = facts_stats_section.find_all(\\'div\\', class_=\\'col-md-8\\')\\n\\nfor i in range(len(categories)):\\n    category = categories[i].text.strip()\\n    values_list = values[i].find_all(\\'li\\')\\n    category_data = {\"category\": category}\\n\\n    for value in values_list:\\n        label = value.find(\\'strong\\').text.strip()\\n        content = value.find(\\'span\\').text.strip()\\n        category_data[label] = content\\n\\n    data.append(category_data)\\n\\nelse:\\n    print(\"Couldn\\'t find the facts and stats section on the webpage.\")\\nfacts_stats_section = soup.find(\\'section\\', class_=\\'facts-stats\\')\\n\\n\\n# Save the data as a JSON file\\nwith open(\\'bu_facts_stats.json\\', \\'w\\') as file:\\n    json.dump(data, file, indent=4)\\n\\nprint(\"Data scraped and saved as bu_facts_stats.json\")\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'http://www.bu.edu/president/boston-university-facts-stats/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status Code: {response.status_code}\")\n",
    "\n",
    "# Find the section containing the facts and stats\n",
    "if facts_stats_section:\n",
    "    # Extract the data from the section\n",
    "data = []\n",
    "categories = facts_stats_section.find_all('div', class_='col-md-4')\n",
    "values = facts_stats_section.find_all('div', class_='col-md-8')\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    category = categories[i].text.strip()\n",
    "    values_list = values[i].find_all('li')\n",
    "    category_data = {\"category\": category}\n",
    "\n",
    "    for value in values_list:\n",
    "        label = value.find('strong').text.strip()\n",
    "        content = value.find('span').text.strip()\n",
    "        category_data[label] = content\n",
    "\n",
    "    data.append(category_data)\n",
    "\n",
    "else:\n",
    "    print(\"Couldn't find the facts and stats section on the webpage.\")\n",
    "facts_stats_section = soup.find('section', class_='facts-stats')\n",
    "\n",
    "\n",
    "# Save the data as a JSON file\n",
    "with open('bu_facts_stats.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Data scraped and saved as bu_facts_stats.json\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39murl = 'https://www.bu.edu/president/boston-university-facts-stats/'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mthe contents of the url has changed and the table containing the scrapped data in the question is no longer available\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "url = 'https://www.bu.edu/president/boston-university-facts-stats/'\n",
    "the contents of the url has changed and the table containing the scrapped data in the question is no longer available\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'https://www.bu.edu/president/boston-university-facts-stats/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the section containing the facts and stats\n",
    "facts_stats_section = soup.find('section', class_='facts-stats')\n",
    "\n",
    "# Extract the data from the section\n",
    "data = {}\n",
    "for row in facts_stats_section.find_all('div', class_='col-md-6'):\n",
    "    key = row.find('h2').text.strip()\n",
    "    value = row.find('p').text.strip()\n",
    "    data[key] = value\n",
    "\n",
    "# Save the data as a JSON file\n",
    "with open('bu_facts_stats.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Data scraped and saved as bu_facts_stats.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Send a GET request to the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the presidents table by its CSS class\n",
    "table = soup.find(\"table\", class_=\"wikitable\")\n",
    "\n",
    "# Initialize an empty list to store the presidents' data\n",
    "presidents = []\n",
    "\n",
    "# Iterate over the rows of the table, skipping the header row\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    # Extract the data from each column\n",
    "    columns = row.find_all(\"td\")\n",
    "    number = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    lifespan = columns[2].text.strip()\n",
    "    term = columns[3].text.strip()\n",
    "    party = columns[4].text.strip()\n",
    "    portrait_url = columns[5].find(\"img\")[\"src\"] if columns[5].find(\"img\") else None\n",
    "\n",
    "    # Create a dictionary to store the president's data\n",
    "    president_data = {\n",
    "        \"number\": number,\n",
    "        \"name\": name,\n",
    "        \"lifespan\": lifespan,\n",
    "        \"term\": term,\n",
    "        \"party\": party,\n",
    "        \"portrait_url\": portrait_url\n",
    "    }\n",
    "\n",
    "    # Add the president's data to the list\n",
    "    presidents.append(president_data)\n",
    "\n",
    "# Convert the presidents list to JSON\n",
    "presidents_json = json.dumps(presidents, indent=4)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"presidents.json\", \"w\") as file:\n",
    "    file.write(presidents_json)\n",
    "\n",
    "print(\"Data scraped and saved as presidents.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mHere the webaite has also changed and the said information no longer availbale on it. \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/umarkiri/Desktop/ArewaDS_WorkSpace/ArewaDS_30Days_Assignment/Day_022/web_scrapping.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here the webaite has also changed and the said information no longer availbale on it. \n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table by its CSS class\n",
    "table = soup.find('table', {'cellpadding': '3'})\n",
    "\n",
    "# Initialize an empty list to store the table data\n",
    "table_data = []\n",
    "\n",
    "# Check if the table is found\n",
    "if table is not None:\n",
    "    # Iterate over the rows of the table\n",
    "    for row in table.find_all('tr'):\n",
    "        # Extract the data from each column\n",
    "        columns = row.find_all('td')\n",
    "        row_data = [column.text.strip() for column in columns]\n",
    "        table_data.append(row_data)\n",
    "\n",
    "# Convert the table data to JSON\n",
    "table_json = json.dumps(table_data)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open('table_data.json', 'w') as file:\n",
    "    file.write(table_json)\n",
    "\n",
    "print('Table data extracted and saved as table_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport json\\n\\n# Send a GET request to the URL\\nurl = \\'https://www.bu.edu/president/boston-university-facts-stats/\\'\\nresponse = requests.get(url)\\n\\n# Create a BeautifulSoup object to parse the HTML content\\nsoup = BeautifulSoup(response.content, \"html.parser\")\\n\\n# Find the table by its CSS class\\ntable = soup.find(\"table\", class_=\"facts-categories\")\\n\\n# Initialize an empty list to store the data\\nfacts_stats = []\\n\\n# Iterate over the rows of the table, skipping the header row\\nfor row in table.find_all(\"tr\")[1:]:\\n    # Extract the data from each column\\n    columns = row.find_all(\"td\")\\n    metric = columns[0].text.strip()\\n    value = columns[1].text.strip()\\n\\n    # Create a dictionary to store the metric and its corresponding value\\n    metric_data = {\\n        \"Metric\": metric,\\n        \"Value\": value\\n    }\\n\\n    # Add the data to the list\\n    facts_stats.append(metric_data)\\n\\n# Convert the list to JSON\\nfacts_stats_json = json.dumps(facts_stats, indent=4)\\n\\n# Save the JSON data to a file\\nwith open(\"facts_stats.json\", \"w\") as file:\\n    file.write(facts_stats_json)\\n\\nprint(\"Data scraped and saved as facts_stats.json\")\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = 'https://www.bu.edu/president/boston-university-facts-stats/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table by its CSS class\n",
    "table = soup.find(\"table\", class_=\"facts-categories\")\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "facts_stats = []\n",
    "\n",
    "# Iterate over the rows of the table, skipping the header row\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    # Extract the data from each column\n",
    "    columns = row.find_all(\"td\")\n",
    "    metric = columns[0].text.strip()\n",
    "    value = columns[1].text.strip()\n",
    "\n",
    "    # Create a dictionary to store the metric and its corresponding value\n",
    "    metric_data = {\n",
    "        \"Metric\": metric,\n",
    "        \"Value\": value\n",
    "    }\n",
    "\n",
    "    # Add the data to the list\n",
    "    facts_stats.append(metric_data)\n",
    "\n",
    "# Convert the list to JSON\n",
    "facts_stats_json = json.dumps(facts_stats, indent=4)\n",
    "\n",
    "# Save the JSON data to a file\n",
    "with open(\"facts_stats.json\", \"w\") as file:\n",
    "    file.write(facts_stats_json)\n",
    "\n",
    "print(\"Data scraped and saved as facts_stats.json\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
